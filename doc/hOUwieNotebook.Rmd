---
title: "hOUwie Notebook"
author: "James Boyko"
date: "`r Sys.Date()`"
output:
  html_document:
    df_print: paged
og:
  type: article
  title: opengraph title
  url: optional opengraph url
  image: optional opengraph image link
footer:
- content: '[link1](http://example.com/) • [link2](http://example.com/)<br/>'
- content: Copyright blah blah
---
```{r, include=FALSE}
source("~/2020_hOUwie/hOUwie.R")
require(OUwie)
require(corHMM)
require(parallel)
```

## Section 1: Introducing hOUwie

### 1.1: Motivation

Many evolutionary questions involve addressing how rates of character evolution change through time either agnostically (Venditti et al. 2006; Harmon et al. 2010; Eastman et al. 2011) or dependent on a particular explanatory variable (O’Meara et al. 2006; May and Moore 2020). Typically, these questions follow a structure which posits that the rate of a continuous character is dependent on state of a discrete character. For example, it could be that the beak length of a Galapagos island finch is dependent on the presence or absence of a drought (Grant and Grant 2006), or that the genome size of a particular plant lineage depends on whether they are a short-lived herbaceous organism or a long-lived woody organism (Beaulieu et al. 2012). In these examples and many others, the evolution of discrete and continuous traits are not independent from each other and are therefore subject to the biases of existing methodology. hOUwie seeks to model the joint evolution of a continuous character with evolution of the discrete variable. In this way, we are able to pose questions that are more specific to the particular biology of the system and are able to provide users a means to apply their biological expertise in a statistically rigorous framework. 

One of the problems with existing approaches for inferring the relationship between rates of evolutionary change and their underlying cause is systematic bias (Revell 2013; May and Moore 2020). In part, this bias exists because current approaches rely on the comparison of null models (models which suggests that there is no rate variation) to alternative models. This dichotomy is susceptible to a straw-man effect in which the influence of a given factor is inflated because it is being compared to an unrealistic and simple null model. To resolve this problem, we propose to develop a model which jointly estimates the rate of continuous character evolution and the evolution of the underlying cause. In addition, our model is able to address the straw-man effect by estimating the presence of hidden rate variation, i.e., variation that is not due to the proposed explanatory variable. 

### 1.2 Goals
Our goal is to identify the principal determinants of model-selection power and parameter-estimation accuracy in hOUwie. We will call these (1) the problem of parameter estimation, and (2) the problem of model selection. In the following, we first describe and explain the model. Next, we outline the design of our simulation study. We then present results regarding the determinants of model-selection power (probability of choosing the correct model) and parameter-estimation accuracy (expected error in estimated parameters). Finally, the inclusion of hidden states adds utility and complexity. With hidden states we gain access to a type of null model, where rate variation in the continuous character can be independent of observed state and is instead dependent on the hidden state. Hidden states also allow for a more complicated model structure. We will discuss and test these dynamics in detail. 

## Section 2: Evaluating hOUwie

### 2.1 The Model

#### The model of continuous character evolution

Our model is composed of two processes: one describes the evolution of a discrete character and the other the evolution of a continuous character. To model the evolution of our continuous character we have chosen to use an Ornstein-Uhlenbeck (OU) model (Hansen 1997; Butler and King 2004, Hansen et al. 2008, Beaulieu et al. 2012). This model combines the stochastic evolution of a trait through time with a deterministic component which models the tendency for a trait to evolve towards an adaptive optima. In this model, a trait ($X(t)$) is is pulled towards an optimum at a rate that is scaled by the parameter $\alpha$, while the optimum itself (which may change through time) is denoted by the parameter $\theta(t)$. $\theta(t)$ is piecewise constant on intervals and takes values in a finite set $\{\theta_i\}$. This represents the set of "selective regimes", "regimes", or Simpson's "adaptive zones" based on narrational preference. Additionally, random deviations are introduced by Gaussian white noise $dB(t)$, which is distributed as a normal random variable with mean zero and variance one. The magnitude of these deviations is scaled by the noise intensity $\sigma$. $\alpha$ has been interpreted as the strength of selection (Simpson 1953, Lande 1976, 1980) and $\sigma$ has been referred to as genetic drift (Lande 1976, Hansen 1997). This latter interpretation has been criticized on the grounds that stochasticity may arise from factors other than genetic drift. The OU process is an Itô difussion satisfying:

$$
X(t) = \alpha(\theta(t) -X(t))+\sigma dB(t)
$$

#### The model of discrete character evolution

Most previous phylogenetic comparative models of an OU process have assumed that the intervals and regimes are known, though the optimum trait values associated with each regime are not (but see, Revell 2013 and May and Moore 2020). This leaves no room for inference about the regimes themselves and how they change through time, nor the possibility that the evolution of the continuous character could influence change in the regimes or vice versa. To resolve this problem, we will model the evolution of these regimes as a discrete character. For this, we have assumed that regime change follows a hidden Markov model (Felsenstein and Churchill 1996; Yang 1994; Beaulieu et al. 2013). 

Hidden Markov models have a hierarchical structure that can be broken down into two components: a "state-dependent process" and an unobserved "parameter process" (Zucchini, MacDonald, & Langrock, 2017). Under an HMM, observations are generated by a given state-dependent process, which in turn depends on the state of the parameter process. In other words, the observed data are the product of several processes occurring in different parts of a phylogeny and the parameter process is way of linking them. It is initially unknown what the parameter process corresponds to biologically, hence the moniker “hidden” state. Nevertheless, the information for detecting hidden states comes from the differences in how the observed states change. As long as the transitions between observed states of different lineages are more adequately described by several Markov processes rather than a single process, there will be information to detect hidden states (Boyko and Beaulieu 2020). In comparative biology, for characters that take on discrete states the standard "state-dependent process" is a continuous-time Markov chain with finite state-space (CTMC-FS). The observed states could be any discretized trait such as presence or absence of extrafloral nectaries (Marazzi et al., 2012), woody or herbaceous growth habit (Beaulieu et al., 2013), or diet state across all animals (Román-Palacios et al. 2019). However, a simple Markov process that assumes homogeneity through time and across taxa is often not adequate to capture the variation of real datasets (e.g. Beaulieu et al., 2013), and thus one way to use HMMs is to link multiple evolutionary models (such as a standar Mk) together. 

Under a standard Mk model, transitions between discrete states occurs at a particular rate $q_{ij}$, where a state changes from state $i$ to state $j$. These transition rates are often presented in the form of a matrix (transition rate matrix). This matrix describes the possible transitions between all $k$ states of the system. For example, an Mk model with $k=3$ states has the following form as it's most general form:

$$
Q_{Mk} = \begin{bmatrix}
- & q_{12} & q_{13} \\
q_{21} & - & q_{23}\\
q_{31} & q_{32} & - \\
\end{bmatrix}
\quad
$$
The $-$ on the diagonal is a quantity required to make each row sum to zero. The Mk modeling framework can be modified and expanded in several ways. These modifications can lead to hypotheses of, for example, ordered transitions (e.g. it may not be possible to directly go from $1 \rightarrow 3$, but instead requires $1 \rightarrow 2 \rightarrow 3$), or models of correlated evolution where the transition rate of character $X$ is dependent the state of character $Y$. Regardless of the state-dependent process, we can link these together in a way that allows for rate variation. For example, say rate matrix $Q_{ord}$ described the evolution of a particular as being ordered and $Q_{er}$ modeled the possibility that all transitions between the states of a particular character are equal. We can combine these different state-dependent structures in a hidden Markov model generally as a block matrix.

$$
Q_{hmm} = \begin{bmatrix}
Q_{ord} & q_{Q_{ord}\rightarrow Q_{er}} \\
q_{Q_{er}\rightarrow Q_{ord}} & Q_{er} \\
\end{bmatrix}
\quad
$$

#### Combining discrete and continuous character evolution

Here we get to the main goal of this project, which is to design a framework for simultaneously optimizing a discrete character model while also fitting a continuous character model with OUwie. The idea behind this is that we assume a correlation between the two trait types, but we reconstruct the regimes first, then fit OUwie separately. However, this is not what we want, because rates of continuous character evolution may not be optimal without information from how the regimes evolved and regime change may not be optimal without information from how rates change. Here we use a hierarchical maximum likelihood approach using the stochastic mapping now implemented in `corHMM()`. The likelihood function is computed by summing over all possible maps.

$$L(X,D|\Theta,\Phi) = P(D|\Phi)\sum_{z}{P(z|D,\Phi)g(X|\Theta, z)},$$
where $P(D|\Phi)$ is the probability of observing the vector states, *D*, at the tips of a phylogeny given parameters $\Phi$, and $P(z|D,\Phi)$ provides the probability of a character map *z* given a vector of character state D and parameters $\Phi$ (note we use $\Phi$ to generically define any model that will estimate a character history). The mixture probability, $g(X|\Theta,z)$, is the BM or OU likelihood given a set of parameters $\Theta$ and character map *z*. 

It is unclear how one would go about calculating $P(z|D,\Phi)$, especially with a larger tree and many character states. Instead, it is easier to sample a large number of realizations of the character transitions given *D* and $\Phi$. This is where the stochastic maps come in. The summation above would then be over a set of unique character maps that appear in the sample. The frequency of each unique map, $f_z$, would then be used as an estimate of $P(z|D,\Phi)$. If each character map is unique, then the summation is simple an average of likelihoods across the set of maps. This approach is costly from a computational perspective because for every new set of $\Phi$ proposed a new set of character maps would have to be generated and reevaluated. 

To start, it is unclear how many maps would suffice to get a good approximation of $P(z|D,\Phi)$. To test this I simulated a birth-death tree (b=1, d=0.5) of 250 tips and scaled tree height to 1. I then simulated a discrete dataset and simmap following the an equal-rates Mk model at rates of $q=0.25, q=1, q=4$. An OUM-type OU model was simulated with $\alpha=0.5, \sigma^2=0.5, \Delta\theta=1$. The true parameter values were then refit 500 times for each Mk rate value to get a sense of how many simmaps is necessary before the same set of parameters produce the same likelihood. 

```{r, echo=FALSE, cache=TRUE}
Rsaves <- dir("~/2020_hOUwie/likConverge/", full.names = TRUE)
rates <- unique(unlist(lapply(strsplit(Rsaves, "-"), function(x) x[3])))
maps <- sort(unique(unlist(lapply(strsplit(Rsaves, "-"), function(x) x[2]))))[c(3,2,5,1,4)]


i = j = 1
fin <- c()
par(mfrow=c(1,3))
for(i in 1:length(rates)){
  ToLoad <- Rsaves[grep(rates[i], Rsaves)]
  dat <- NA
  for(j in 1:length(maps)){
    load(ToLoad[grep(maps[j], ToLoad)])
    # dat <- cbind(dat, unlist(lapply(out, function(x) x$loglik)))
    dat <- cbind(dat, unlist(out))
  }
  dat <- dat[,-1]
  colnames(dat) <- gsub("simmaps", "", maps)
  fin <- rbind(fin, dat)
  boxplot(dat, main = rates[i], xlab = "no. of simmaps", ylab = "llik")
}
```
So far as I know there are not any guidelines for what the variance of a non-deterministic MLE should be. However, 100 simmaps seems to reduce the variance of the estimate enough to be considered a sufficient estimate of $\hat\theta$. There is the potential to take a hybrid estimation approach utilizing the computation faster 10 simmaps to locate a global optimum and 100 simmaps (or more) to find the local optimum. Any advantages this approach could have would be dependent on the "sharpness" likelihood surface and thus require us to examine the likelihood surface across many datasets.

### 2.2 The Simulation Study

#### 2.2.1 A brief aside discussing the parameters of the hOUwie model

The maximum number of parameters ($k$) for the most complex hOUwie model is going to depend on the number of observed discrete states and the number of desired hidden rate categories (for the global model, the OU portion of hOUwie will always be OUMVA and therefore the number of parameters will be determined by the number of regimes). 

$$
\\k_{hOUwie} = 3*k_{obs}*k_{hid} + k_{hmm}
$$

Where $k_{obs}$ is the number of observed states, $k_{hid}$ is the number of hidden rate categories, $k_{hmm} = k_{Mk}*k_{hid} + k_{hid}^2 - k_{hid}$ is the number of parameters in a hidden Markov model, and $k_{Mk} = k_{obs}^2 - k_{obs}$ is the number of parameters in an Mk model. For example, the global model if we had $k_{obs}=3$ and $k_{hid}=2$ would be:

```{r, echo=FALSE}
cat("The HMM rate matrix\n")
rate.mat <- getFullMat(list(getRateCatMat(3), getRateCatMat(3)), getRateCatMat(2))
print(rate.mat)
cat("\nThe OU index matrix\n")
ou.mat <- getOUParamStructure("OUMVA", "three.point", FALSE, FALSE, dim(rate.mat)[1])
print(ou.mat)
cat("\n", max(rate.mat, na.rm = TRUE) + max(ou.mat, na.rm = TRUE), "total params.\n")
```

With such complexity possible for just 3 observed states and 2 hidden states we cannot reasonably explore all parameter space and all of the potential model structures nested within the global model. Instead, we will follow Beaulieu and O'Meara (2016) and split the set of models into two groups. Group one will evaluate the performance of hOUwie by simulating data under various scenarios and then estimating the fit and bias of inferred rates. Group two will evaluate hOUwie's character independent model which can detect rate variation that is not associated with the observed character. 

#### 2.2.2.2 Group One

We will fit hOUwie and two alternative models based on approaches an empiricist may take: (1) TwoStep - maximize corHMM and OUwie separately (data analyzed completely separately). (2) NonCensored - maximize corHMM, simulate $n_{map}$ simmaps based on corHMM's $\hat\theta$, fit OUwie to the maps, summarize OUwie. (3) hOUwie - jointly maximize OUwie and corHMM with $n_{map}$ iterations.

##### An Mk BMS model (sensu Revell 2013).
```{r, echo=FALSE}
cat("The Mk rate matrix\n")
rate.mat <- getRateCatMat(2)
rate.mat <- equateStateMatPars(rate.mat, c(1,2))
print(rate.mat)
cat("\nThe OU index matrix\n")
ou.mat <- getOUParamStructure("BMS", "three.point", FALSE, FALSE, dim(rate.mat)[1])
print(ou.mat)
cat("\n", max(rate.mat, na.rm = TRUE) + max(ou.mat, na.rm = TRUE), "total params.\n")
```

This model framework was used by Revell (2013) to demonstrate that using stochastic maps to estimate variable rate $\sigma_i^2$ (where $i$ is a regime specific rate parameter) was biased. Specifically, he found that the regime specific $\sigma^2$ were biased to be more similar towards one another than the true simulating values. I replicated this study simulating 500 datasets per single phylogeny ($b=1, d=0.5, nTip=100, H=1$). The simulations followed Revell (2013) where $\sigma_1^2 = 1$ and $\sigma_2^2 = 10$. We evaluated only the higher Mk rates $q=1, q=2, q=4, q=8$ since Revell (2013) found no bias at lower rates.

```{r, echo=FALSE, cache=TRUE, fig.width=8, fig.height=6}
organizeSimulators <- function(simulators){
  k.cor <- max(simulators$index.cor, na.rm = TRUE) - 1 # number of corhmm params
  k.ou <- max(simulators$index.ou, na.rm = TRUE) - 1 # number of ouwie params
  p.mk <- simulators$pars[1:k.cor]
  p.ou <- simulators$pars[(k.cor+1):length(simulators$pars)]
  simulators$index.cor[simulators$index.cor == 0] <- NA
  simulators$index.cor[is.na(simulators$index.cor)] <- max(simulators$index.cor, na.rm = TRUE) + 1
  Q <- matrix(0, dim(simulators$index.cor)[1], dim(simulators$index.cor)[1])
  Q[] <- c(p.mk, 0)[simulators$index.cor]
  diag(Q) <- -rowSums(Q)
  Rate.mat <- matrix(1, 3, dim(simulators$index.cor)[2])
  simulators$index.ou[is.na(simulators$index.ou)] <- max(simulators$index.ou, na.rm = TRUE) + 1
  Rate.mat[] <- c(p.ou, 1e-10)[simulators$index.ou]
  rownames(Rate.mat) <- c("alpha", "sigma.sq", "theta")
  return(list(pars.cor = Q,
              pars.ou = Rate.mat))
}
sub.folders <- dir("~/2020_hOUwie/ModelTesting/ER_BMS", full.names = TRUE)
MSE.tables <- Bias.tables <- Var.tables <- vector("list", length(sub.folders))
par(mfrow=c(2,2))
for(i in 1:length(sub.folders)){
  Rsaves <- dir(sub.folders[i], full.names = TRUE)
  big.obj <- list()
  for(j in 1:length(Rsaves)){
    load(Rsaves[j])
    big.obj[[j]] <- obj
  }
  truepars <- organizeSimulators(obj$data.houwie)
  
  ## sigma squared ratio
  sig2.TwoStep <- do.call(rbind, lapply(big.obj, function(x) x$TwoStepFit$solution.ou[2,]))
  sig2.NonCens <- do.call(rbind, lapply(big.obj, function(x) x$NonCensFit$solution.ou[2,]))
  sig2.hOUwie <- do.call(rbind, lapply(big.obj, function(x) x$hOUwieFit$solution.ou[2,]))
  sig2.TruMap <- do.call(rbind, lapply(big.obj, function(x) x$TruMapFit$solution[2,]))
  
  sig2.ratio.table <- data.frame(TwoStep=sig2.TwoStep[,2]/sig2.TwoStep[,1],
                                 NonCens=sig2.NonCens[,2]/sig2.NonCens[,1],
                                 hOUwie=sig2.hOUwie[,2]/sig2.hOUwie[,1],
                                 TrueMap=sig2.TruMap[,2]/sig2.TruMap[,1])
  sig2.ratio.table <- sig2.ratio.table[!apply(sig2.ratio.table, 1, function(x) any(x > 100)),]
  
  sig2.resid.table <- data.frame(TwoStepSig1=sig2.TwoStep[,1] - truepars$pars.ou[2,1],
                                 TwoStepSig2=sig2.TwoStep[,2] - truepars$pars.ou[2,2],
                                 NonCensSig1=sig2.NonCens[,1] - truepars$pars.ou[2,1],
                                 NonCensSig2=sig2.NonCens[,2] - truepars$pars.ou[2,2],
                                 hOUwieSig1=sig2.hOUwie[,1] - truepars$pars.ou[2,1],
                                 hOUwieSig2=sig2.hOUwie[,2] - truepars$pars.ou[2,2],
                                 TrueMapSig1=sig2.TruMap[,1] - truepars$pars.ou[2,1],
                                 TrueMapSig2=sig2.TruMap[,2] - truepars$pars.ou[2,2])
  
  
  ## mean squared error (MSE)
  ## E(thet_hat - theta)^2
  MSE.tables[[i]] <- data.frame(
    MSE.Sig1 = c(mean((sig2.TwoStep[,1] - truepars$pars.ou[2,1])^2),
                mean((sig2.NonCens[,1] - truepars$pars.ou[2,1])^2),
                mean((sig2.hOUwie[,1] - truepars$pars.ou[2,1])^2),
                mean((sig2.TruMap[,1] - truepars$pars.ou[2,1])^2)),
    MSE.Sig2 = c(mean((sig2.TwoStep[,2] - truepars$pars.ou[2,2])^2),
                mean((sig2.NonCens[,2] - truepars$pars.ou[2,2])^2),
                mean((sig2.hOUwie[,2] - truepars$pars.ou[2,2])^2),
                mean((sig2.TruMap[,2] - truepars$pars.ou[2,2])^2)),
    row.names = c("TwoStep", "NoneCens", "hOUwie", "TrueMap")
  )
  
  ## calculate the bias
  ## E(theta_hat) - theta
  ## E(theta_hat - theta) (i.e. the expectation of the error)
  Bias.tables[[i]] <- data.frame(
    B.Sig1 = c(mean(sig2.TwoStep[,1]) - truepars$pars.ou[2,1],
                mean(sig2.NonCens[,1]) - truepars$pars.ou[2,1],
                mean(sig2.hOUwie[,1]) - truepars$pars.ou[2,1],
                mean(sig2.TruMap[,1]) - truepars$pars.ou[2,1]),
    B.Sig2 = c(mean(sig2.TwoStep[,2]) - truepars$pars.ou[2,2],
                mean(sig2.NonCens[,2]) - truepars$pars.ou[2,2],
                mean(sig2.hOUwie[,2]) - truepars$pars.ou[2,2],
                mean(sig2.TruMap[,2]) - truepars$pars.ou[2,2]),
    row.names = c("TwoStep", "NoneCens", "hOUwie", "TrueMap")
  )
  
  ## calculate the variance 
  Var.tables[[i]] <- data.frame(
    Var.Sig1 = c(var(sig2.TwoStep[,1]),
                var(sig2.NonCens[,1]),
                var(sig2.hOUwie[,1]),
                var(sig2.TruMap[,1])),
    Var.Sig2 = c(var(sig2.TwoStep[,2]),
                var(sig2.NonCens[,2]),
                var(sig2.hOUwie[,2]),
                var(sig2.TruMap[,2])),
    row.names = c("TwoStep", "NoneCens", "hOUwie", "TrueMap")
  )
  
  boxplot(sig2.ratio.table,
          ylab = expression(paste(sigma [2] ^2 / sigma [1] ^2 )),
          xlab = "Model used to estimate 500 datasets",
          ylim = c(0, 40),
          main = paste("q =", truepars$pars.cor[1,2]))
  abline(h = truepars$pars.ou[2,2]/truepars$pars.ou[2,1], col = "red")
}
```
Matching what had been found previously, at lower transition rates $\sigma_i^2$ is estimated well, but at higher rates there is bias underestimating the ratio of $\hat\sigma_1^2/\hat\sigma_2^2$ for most methods. This is true of hOUwie also, but it is less severe. 

Furthermore, we can get exact measures of the variance, $Var(\hat\theta)$, bias, $E(\hat\theta) -\theta$, and mean squared error, $MSE(\hat\theta) =E(\hat\theta -\theta)^2 = Var(\hat\theta)+B^2$. An unbiased estimator (an important quality for any point estimator) is one which bias goes to zero as more data is given. In general, if two estimators are both unbiased, it is better to choose the one with the lowest mean squared error. Below I present this information for each Mk rate in table form.

```{r, echo=FALSE}
ErrTable <- rbind(
  cbind(rate=1, cbind(MSE.tables[[1]], Bias.tables[[1]], Var.tables[[1]])),
  cbind(rate=2, cbind(MSE.tables[[2]], Bias.tables[[2]], Var.tables[[2]])),
  cbind(rate=4, cbind(MSE.tables[[3]], Bias.tables[[3]], Var.tables[[3]])),
  cbind(rate=8, cbind(MSE.tables[[4]], Bias.tables[[4]], Var.tables[[4]]))
)
print(round(ErrTable, 3))
```

Although hOUwie does not always have lower MSEs than the alternative models, it significantly improves the bias for all models and rates and MSE differences are caused by increased $Var(\hat\theta)$ for hOUwie than other models. This is likely due to hOUwie estimating more parameters for any given iteration than our alternative models. 

An Mk OUM model 
```{r, echo=FALSE}
cat("The Mk rate matrix\n")
rate.mat <- getRateCatMat(2)
rate.mat <- equateStateMatPars(rate.mat, c(1,2))
print(rate.mat)
cat("\nThe OU index matrix\n")
ou.mat <- getOUParamStructure("OUM", "three.point", FALSE, FALSE, dim(rate.mat)[1])
print(ou.mat)
cat("\n", max(rate.mat, na.rm = TRUE) + max(ou.mat, na.rm = TRUE), "total params.\n")
```
An Mk OUMVA model 
```{r, echo=FALSE}
cat("The Mk rate matrix\n")
rate.mat <- getRateCatMat(2)
rate.mat <- equateStateMatPars(rate.mat, c(1,2))
print(rate.mat)
cat("\nThe OU index matrix\n")
ou.mat <- getOUParamStructure("OUMVA", "three.point", FALSE, FALSE, dim(rate.mat)[1])
print(ou.mat)
cat("\n", max(rate.mat, na.rm = TRUE) + max(ou.mat, na.rm = TRUE), "total params.\n")
```
An HMM BMS model
```{r, echo=FALSE}
cat("The HMM rate matrix\n")
rate.mat <- getRateCatMat(2)
rate.mat <- equateStateMatPars(rate.mat, c(1,2))
rate.mat <- getFullMat(list(rate.mat, rate.mat), rate.mat)
print(rate.mat)
cat("\nThe OU index matrix\n")
ou.mat <- getOUParamStructure("BMS", "three.point", FALSE, FALSE, dim(rate.mat)[1])
print(ou.mat)
cat("\n", max(rate.mat, na.rm = TRUE) + max(ou.mat, na.rm = TRUE), "total params.\n")
```
An HMM OUM model
```{r, echo=FALSE}
cat("The HMM rate matrix\n")
rate.mat <- getRateCatMat(2)
rate.mat <- equateStateMatPars(rate.mat, c(1,2))
rate.mat <- getFullMat(list(rate.mat, rate.mat), rate.mat)
print(rate.mat)
cat("\nThe OU index matrix\n")
ou.mat <- getOUParamStructure("OUM", "three.point", FALSE, FALSE, dim(rate.mat)[1])
print(ou.mat)
cat("\n", max(rate.mat, na.rm = TRUE) + max(ou.mat, na.rm = TRUE), "total params.\n")
```
An HMM OUMVA model 
```{r, echo=FALSE}
cat("The HMM rate matrix\n")
rate.mat <- getRateCatMat(2)
rate.mat <- equateStateMatPars(rate.mat, c(1,2))
rate.mat <- getFullMat(list(rate.mat, rate.mat), rate.mat)
print(rate.mat)
cat("\nThe OU index matrix\n")
ou.mat <- getOUParamStructure("OUMVA", "three.point", FALSE, FALSE, dim(rate.mat)[1])
print(ou.mat)
cat("\n", max(rate.mat, na.rm = TRUE) + max(ou.mat, na.rm = TRUE), "total params.\n")
```

A weakness of this set of models is that most of our complexity comes from the OU process (more free parameters). Boyko and Beaulieu (2020) demonstrated that the power for inferring an HMM comes predominantly from the difference between state-dependent processes. One interesting part of hOUwie is that it gives an additional axis to distinguish between state-dependent processes. Standard HMMs distinguish between state-dependent processes based on differences in how observed states change (eg. rate class A changes quickly between woody and herbacious while rate class B changes slowly). With hOUwie, we add information about how the continuous trait changes in distinguishing between the state-dependent processes. This point forms the basis of our null model, but more about that later.



#### Group Two

Here we outline our questions and how we will address the statistical behavior of hOUwie. Some questions we have already alluded to, such as how many simmaps are necessary for a good approximation of $P(z|D,\Phi)$? However, we have yet to outline the general approach and more question-specific methods that will utilize. 



### 2.3 The Problem of Parameter Estimation

### 2.4 The Problem of Model Selection


## Section 3: Discussing hOUwie

Key interpretations will be presented here.

> Typically, the selection of models is not an end in itself but is rather an aid to reasoning about evolutionary mechanisms and drivers. In such a context, one bears in mind that the larger the number of models entertained, the less reliable are the inferences obtained. This is a problem that the use of information criteria only incompletely ameliorates (Boettiger et al. 2012; Ho and Ané 2014). For this reason, inferences are often clearest when the hypotheses are laid out a priori. 

This is exasperated in a model like hOUwie where the potential models that can be analyzed is so large.

> Regardless of the ultimate goal, however, greater flexibility and realism in models comes at the cost of more mathematical complexity, more parameters to estimate, and more models to compare. In effect, as parameter space dimension increases, the information contained in data becomes more dilute, the precision with which parameters can be estimated diminishes, and the risk of overfitting grows (Ho and Ané 2014).

Can we get support for an HMM when the only difference is in the continuous process?

Where some models preform well (low rates NonCensured and hOUwie preform well) would hOUwieNull throw change the behavior? Could we see biases in the params?

If a continuous character has rate variation that is separate from the observed character will hOUwie provide a false positive? Does a null model fix this problem?

> The foregoing observations have implications for more parameter-rich methods based on generalizations of the OUCH class in which parameters are allowed to vary across the tree (O’Meara et al. 2006; Beaulieu et al. 2012). Our study shows that the information contained in the data are already markedly limited when these parameters are constant. Increasing the number of parameters can only further dilute information, leading to even greater uncertainty in estimates. 

Can estimating the regimes help with this or are we going too far?

Addressing the curse of dimensionality.

Can we improve the predictive power of our model by making it more complex? This will come at the cost of inferential accuracy. 

Can we quantify the amount of information a discrete character contains about the continuous variable?

Can we recover the lack of phylogenetic signal (as alpha goes to infinity) with the inclusion of the underlying discrete process that alpha is linked to? Does this create biases in other models that hOUwie resolves? High alpha associated with a Markov model means that the phylogenetic signal is determined by the Markov model itself (retention index as a possible way to determine signal - although actually used to measure homoplasy).

hOUwie with missing data. Can we generate predictions of missing data with hOUwie? The model only has continuous variable or model only has discrete variable, predict the other.


```{r, echo=FALSE}
index.BMS <- getOUParamStructure("BMS", "three.point", FALSE, FALSE, 2)
index.OUM <- getOUParamStructure("OUM", "three.point", FALSE, FALSE, 2)
index.OUMVA <- getOUParamStructure("OUMVA", "three.point", FALSE, FALSE, 2)
rownames(index.BMS) <- rownames(index.OUM) <- rownames(index.OUMVA) <- 
cat("2-state BMS model.\n")
print(index.BMS)
cat("2-state OUM model.\n")
print(index.BMS)
cat("2-state OUMVA model.\n")
print(index.BMS)
```

Model evaluation
Power Analysis.
PBIAS?
MSE?


## May and Moore 2020
We begin by developing a stochastic process that explicitly models the joint evolution of the discrete and continuous characters; this stochastic process can accommodate one or more continuous characters evolving under a state-dependent multivariate Brownian motion process. 

We then develop an inference model that accommodates variation in the background rate of continuous-character evolution (i.e., rate variation across lineages that is independent of the discrete character under consideration).

We show by simulation that the method is able to reliably infer the state-dependent rates of continuous-character evolution, and that ignoring background-rate variation leads to an inflated false-positive rate. 

They simulate there model as follows: a discrete trait evolves under a continuous-time Markov process, changing from state 0 to state 1 with rate q01, and from state 1 to state 0 with rate q10. The continuous character evolves under a state-dependent Brownian motion process, where the diffusion rate, $s^2_i$ , measures the rate of continuous-character evolution when the discrete character is in state i (i.e., $s^2_1$ > $s^2_0$ indicates that the continuous character evolves faster in discrete state 1 than in discrete state 0). In a small time interval of duration $\Delta t$, where the discrete character begins in state $i$, the continuous character changes by a normally distributed random variable with mean 0 and variance $s^2_i \Delta t$, and the discrete character changes state with probability $q_{ij} \Delta t$.

The transition-probability density specifies the probability that the process ends in some state, ($x_t,y_t$), given an initial state ($x_0,y_0$), after a certain amount of time, $T$, has elapsed. The resulting frequency histogram of end states provides a Monte Carlo approximation of the transition-probability density for a branch of duration $T$.
(Could be a cool visualization tool to examine how our hOUwie sims change stuff.)

We model the joint evolution of a discrete binary character, $X$, and a set of $c$ continuous characters, $Y$, as a stochastic process. The discrete trait has two possible values, which we arbitrarily label 0 and 1; $x ∈ (0,1)$. (There model allows for any number of continuous traits, ours does one. Our model allows for any number of discrete states and characters, theres only does one binary discrete character.)

We assume that the discrete character evolves under a continuous-time Markov process, and that the continuous characters evolve under a multivariate Brownian motion process with rates that depend on the state of the binary character. These model components collectively describe how the set of characters, (x,y), evolve together over a single branch; we detail the evolutionary dynamics of this process over an entire tree when we describe the likelihood function.

The complete history of the discrete trait on branch $l$ — which we represent as $\kappa_l$ — specifies the state of the character at the beginning and end of the branch, and also the times of any state changes along the branch. 

While the process is in a particular discrete state, we assume the continuous characters evolve under a multivariate Brownian motion model. We allow the background rate of evolution to vary among lineages in the phylogeny by letting each branch have its own rate parameter, $\beta^2_l$ (we call this "background-rate variation"). While in discrete state $i$, the background rate is multiplied by the state-specific relative rate, $\zeta^2_i$. We also allow the relative rate of evolution to vary among the continuous characters. The vector $\sigma^2$ contains these relative rates; $\sigma^2_i$ is the relative rate at which continuous character $i$ evolves. The evolutionary correlations between characters are contained in the $c×c$ symmetric correlation matrix, $R$, where $[R]_{ij} = \rho_{ij}$ specifies the correlation between characters $i$ and $j$.


We assume that the relative rates of change between characters, $\sigma^2$, and the evolutionary correlations between characters, $R$, are independent of the discrete state; in other words, we assume that the state of the discrete trait affects only the overall rate of continuous-character evolution, but not the nature of the evolutionary process (as represented by $\sigma^2$ and $R$). We combine the relative rates among characters and the correlation matrix to form the overall evolutionary variance-covariance matrix $\Sigma$:

$$
\Sigma = \begin{bmatrix}
\sigma^2_1 & \sigma_1\sigma_2\rho_{12} & ... & \sigma_1\sigma_c\rho_{1c} \\
\sigma_1\sigma_2\rho_{12} & \sigma^2_2 & ... & \sigma_2\sigma_c\rho_{2c}\\
... & ... & ... & ... \\
\sigma_c\sigma_1\rho_{1c} & \sigma_c\sigma_2\rho_{2c} & ... & \sigma^2_c \\
\end{bmatrix}
\quad
$$
(They have an estimate of the rate of continuous trait evolution that is independent of the discrete character $\sigma^2_i$ and one that is dependent of the discrete character $\zeta_i^2$. Both terms are indexed by $i$, but it seems $\zeta^2_i$ is indexed by the state of the discrete character and $\sigma^2_i$ is indexed by the particular continuous character (they are doing multivariate continuous characters)).

We simplify likelihood calculations by including the vector of character histories, $\kappa$, where $\kappa_l$ is the discrete-character history along branch $l$ (including the state at the beginning and end of the branch), as variables in the model. In effect, we are "augmenting" the discrete-character data observed at the tips of the tree with unobserved discrete-character histories over the entire tree; this technique is referred to as data augmentation

$$
P(\mathcal {X, Y, \kappa} | \Psi, \theta) = P(\mathcal{X, \kappa}| \Psi, \theta)P(\mathcal{Y}|\kappa, \Psi, \theta)
$$

(They calculate the probability of a given simmap, not just assume an equal value for all)



